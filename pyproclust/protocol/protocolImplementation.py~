'''
Created on 21/05/2012

@author: victor
'''
import pyproclust.tools.commonTools as common
from pyproclust.matrix.generators.vmdRMSDGenerator import VmdDistanceMatrixGenerator
import pickle
import pyproclust.tools.pdbTools as pdb_common
from pyproclust.protocol.processPool import ProcessPool
from multiprocessing.process import Process
from pyproclust.tools.distanceMatrixAnalysisTools import statistical_analysis,\
    distance_distribution_analysis, mean_distance_distribution_analysis,\
    kdist_analysis, mean_distance_per_element,\
    percent_of_elements_within_cutoff_analysis

from pyproclust.algorithms.hierarchical.hierarchicalAlgorithm import HierarchicalClusteringAlgorithm
from pyproclust.algorithms.hierarchical.hierarchicalTools import calculate_cutoff_numcluster_list
from pyproclust.algorithms.gromos.gromosAlgorithm import GromosAlgorithm
from pyproclust.algorithms.random.RandomAlgorithm import RandomClusteringAlgorithm
import pyproclust.tools.scriptTools as scripts_common
from pyproclust.algorithms.kmedoids.kMedoids import KMedoids
from pyproclust.tools.pdbTools import extract_frames_from_trajectory
from pyproclust.tools.commonTools import merge_files
from pyproclust.clustering.comparison.comparator import Comparator
from pyproclust.clustering.analysis.picklingParallelAnalysisRunner import PicklingParallelAnalysisRunner
from pyproclust.algorithms.dbscan.dbscanTools import dbscan_param_space_search
from pyproclust.algorithms.dbscan.dbscanAlgorithm import DBSCANAlgorithm
from pyproclust.clustering.analysis.analysisPopulator import AnalysisPopulator
from pyproclust.clustering.filtering.clusteringFilter import ClusteringFilter


class Protocol(object):
    '''
    Implementation of the protocol (Search in the space of clusterings).
    This will:
    
    '''
    def __init__(self):
        pass
        
    def run(self,protocol_params):
        self.condensed_distance_matrix = None
        
        #####################
        # Create subdirectories 
        #####################
        self.create_workspace(protocol_params)
        
        ######################
        # Obtaining the distance matrix
        ######################
        if protocol_params.shallWeCalculateDistanceMatrix():
            print "We are going to calculate the distance matrix."
            self.genMatrix(protocol_params)
        else:
            print "We don't need to calculate the distance matrix because it is provided in:", protocol_params.matrix_path
            self.loadMatrix(protocol_params)
         
        ######################
        # Evaluate distance matrix
        ###################### 
        if protocol_params.perform_distance_matrix_analysis:
            common.print_and_flush("Performing statistical analysis of distance matrix...")
            matrix_analysis_working_dir = protocol_params.working_directory+"/matrix_analysis"
            matrix_props_handler = open(matrix_analysis_working_dir+"/matrix_props.txt","w")
            min_dist, max_dist, mean_dist = statistical_analysis(matrix_props_handler,self.condensed_distance_matrix) #@UnusedVariable
            matrix_props_handler.close()
            common.print_and_flush(" Done\n")
            
            processManager = ProcessPool(protocol_params.number_of_processors,sleep_time = 30)
            spawn_matrix_analysis_processes(self.condensed_distance_matrix,protocol_params,matrix_analysis_working_dir,processManager)
            processManager.consume()
        else:
            common.print_and_flush("We are determining mean and max dist of matrix.\n")
            min_dist, max_dist, mean_dist = statistical_analysis(None,self.condensed_distance_matrix) #@UnusedVariable
            print "Min dist: ",min_dist
            print "Max dist: ",max_dist
            print "Mean dist: ",mean_dist
            
        ######################
        # Clustering exploration
        ######################
        self.do_clustering_exploration(protocol_params, condensed_distance_matrix, max_dist, mean_dist)
        
        ######################
        # Load created clusterings from disk
        ######################
        non_filtered_clusterings = scripts_common.load_binary_clusters("clusterings/")
        
        ######################
        # Filter clusterings
        ######################
        self.do_clustering_filtering()
        
        if len(self.clusterings) == 0:
            print "The clustering search gave no clusterings. Relax noise or num. of clusters parameters."
        else:
            ######################
            # Evaluate all the clusterings
            ######################
            string_results, results_pack = self.clustering_scoring(protocol_params)
            
            ######################
            # Bake up results
            ######################
            self.save_results(protocol_params,string_results,results_pack)
            
            ######################
            # Choose the best one
            ######################
            best_score, best_clustering = protocol_params.chooseBestClustering(results_pack)
            best_cluster_string = "Best clustering has a normalized score of : %.4f Details: %s\n"%(best_score,best_clustering.details)
            print best_cluster_string
            
            if protocol_params.most_representative_pdb_file != "":
                # Save the most representative frames of the chosen clustering
                self.save_most_representative(protocol_params,best_clustering)
            
            if protocol_params.shallWeCompareTrajectories():
                comparator = Comparator()
                comparator.run(best_clustering,protocol_params.pdb1,protocol_params.pdb2,self.condensed_distance_matrix,"results/"+protocol_params.state_graph_path)
                print comparator.result_string 
                if protocol_params.comparator_results_path != "":
                    file_handler = open("results/"+protocol_params.comparator_results_path,"w")
                    file_handler.write(best_cluster_string )
                    file_handler.write(comparator.result_string )
                    file_handler.close()
    
    def do_clustering_filtering(self,protocol_params,non_filtered_clusterings):
        print "We start the filtering with ", len (non_filtered_clusterings),"clusterings"
        cfilter = ClusteringFilter(protocol_params)
        self.clusterings = cfilter.doClusteringFiltering(non_filtered_clusterings,self.condensed_distance_matrix.row_length)
        print "We end with ", len(self.clusterings), "clusterings"
        
    
    def do_clustering_exploration(self, protocol_params, condensed_distance_matrix, max_dist, mean_dist):
        processManager = self.get_algorithm_scheduler(protocol_params)
        
        spawn_gromos(protocol_params, condensed_distance_matrix,processManager)
        
        spawn_random(protocol_params, condensed_distance_matrix,processManager)
        
        spawn_hierarchical(protocol_params, condensed_distance_matrix, max_dist, processManager) 
        
        spawn_dbscan(protocol_params, condensed_distance_matrix,processManager)
        
        spawn_kmedoids(protocol_params, condensed_distance_matrix, mean_dist, processManager)
        
        processManager.consume() # Will wait until all the processes finish

    def get_algorithm_scheduler(self, protocol_params):
        return ProcessPool(protocol_params.number_of_processors, protocol_params.algorithm_scheduler_sleep_time)
    
    def save_results(self,protocol_params,string_results,results_pack):
        if (protocol_params.report_file != ""):
            reports_file_handler = open('results/'+protocol_params.report_file+".txt","w")
            reports_file_handler.write(string_results)
            reports_file_handler.close()
            result_pack_file_handler = open('results/'+protocol_params.report_file+".bin","w")
            pickle.dump(results_pack,result_pack_file_handler)
            result_pack_file_handler.close()
        else:
            print string_results
    
    def clustering_scoring(self,protocol_params):
        analyzer = PicklingParallelAnalysisRunner(protocol_params.number_of_processors,protocol_params.scoring_scheduler_sleep_time)
        AnalysisPopulator(analyzer, self.condensed_distance_matrix,protocol_params.evaluation_types)
        for c in self.clusterings:
            analyzer.run_analysis_for(c)
            
        return  analyzer.generate_report()
    
    def create_workspace(self, protocol_params):
        scripts_common.make_directory(protocol_params.getWorkspacePathFor("results"))
        scripts_common.make_directory(protocol_params.getWorkspacePathFor("matrix analysis"))
        scripts_common.make_directory(protocol_params.getWorkspacePathFor("tmp"))
        scripts_common.make_directory(protocol_params.getWorkspacePathFor("clusterings"))

    def save_most_representative(self,protocol_params,clustering):
        medoids= []
        for c in clustering.clusters:
            medoids.append(c.calculate_medoid(self.condensed_distance_matrix))
        
        temporary_merged_trajectory_path = "tmp/tmp_merged_trajectory.pdb"
        file_handler_in = None
        if protocol_params.shallWeMergetrajectories(): 
            # We want to merge both trajectories and filter them (we're only interested in alphas right now)
            common.print_and_flush("Merging trajectories ...")
            file_handler_in = open(temporary_merged_trajectory_path,"w")
            pdb1_fh = open(protocol_params.pdb1)
            pdb2_fh = open(protocol_params.pdb2)
            merge_files([pdb1_fh,pdb2_fh],file_handler_in,True)
            file_handler_in.close()
            pdb1_fh.close()
            pdb2_fh.close()
            common.print_and_flush(" Done\n")
            file_handler_in = open(temporary_merged_trajectory_path,"r")
        else:
            # We are analyzing only one trajectory
            file_handler_in = open(protocol_params.pdb1)
        
        file_handler_out = open("results/"+protocol_params.most_representative_pdb_file,"w")
        extract_frames_from_trajectory(file_handler_in, self.condensed_distance_matrix.row_length, file_handler_out, medoids)
        file_handler_in.close()
        file_handler_out.close()
        
    def loadMatrix(self,protocol_params):
        common.print_and_flush("Loading condensed matrix...")
        self.condensed_distance_matrix = scripts_common.load_matrix(protocol_params.matrix_path)
        common.print_and_flush(" Done\n")
        

    def matrix_creation(self, protocol_params, matrix_prefix, trajectory_path):
        matrix_generator = VmdDistanceMatrixGenerator(trajectory_path, matrix_prefix, False, protocol_params.fit_selection, protocol_params.rmsd_selection)
        common.print_and_flush("Creating matrix ... \n")
        self.condensed_distance_matrix = matrix_generator.generate_condensed_matrix(protocol_params.number_of_processors, verbose=True)
        common.print_and_flush(" Done\n")

    def genMatrix(self,protocol_params):
        matrix_prefix = "tmp/tmp_matrix"
        trajectory_path = ""
        temporary_merged_trajectory_path = "tmp/tmp_merged_trajectory.pdb"
        if protocol_params.shallWeMergetrajectories(): 
            # We want to merge both trajectories and filter them (we're only interested in alphas right now)
            common.print_and_flush("Merging trajectories...")
            file_handler_out = open(temporary_merged_trajectory_path,"w")
            pdb1_fh = open(protocol_params.pdb1)
            pdb2_fh = open(protocol_params.pdb2)
            if protocol_params.CA_simplification:
                pdb_common.create_CA_file (pdb1_fh, file_handler_out)
                pdb_common.create_CA_file (pdb2_fh, file_handler_out)
            else:
                merge_files([pdb1_fh,pdb2_fh],file_handler_out,True)
            file_handler_out.close()
            pdb1_fh.close()
            pdb2_fh.close()
            common.print_and_flush("Done\n")
            trajectory_path = temporary_merged_trajectory_path
        else:
            # We are analyzing only one trajectory
            if protocol_params.CA_simplification:
                file_handler_out = open(temporary_merged_trajectory_path,"w")
                pdb1_fh = open(protocol_params.pdb1)
                file_handler_out.close()
                pdb1_fh.close()
                trajectory_path = temporary_merged_trajectory_path
            else:
                trajectory_path = protocol_params.pdb1
        
        self.matrix_creation(protocol_params, matrix_prefix, trajectory_path)
        
        ########################################
        # If required write the matrix
        ######################################## 
        if protocol_params.store_matrix_path:
            common.print_and_flush("Writing matrix (in "+protocol_params.store_matrix_path+".bin) ...")
            output_handler = open(protocol_params.store_matrix_path+".bin",'w')
            pickle.dump(self.condensed_distance_matrix,output_handler)
            output_handler.close()
            common.print_and_flush(" Done\n")
        

def spawn_hierarchical(protocol_params, condensed_distance_matrix, max_dist, process_manager):
    ###################
    # Hierarchical
    ###################
    common.print_and_flush("Spawning hierarchical clustering.\n")
    # Calculate cutoffs for hierarchical if necessary
    if len(protocol_params.hierarchical_cutoff_list) == 0: 
        spawn_hierarchical_cutoff_calculation(protocol_params,condensed_distance_matrix, max_dist, process_manager)
    else:
        spawn_hierarchical_with_cutoffs(protocol_params, condensed_distance_matrix, max_dist, process_manager)

def spawn_hierarchical_cutoff_calculation(protocol_params,condensed_distance_matrix, max_dist, process_manager):
    process_name = "Hierarchical_Cutoff_Calculation"
    description = "Hierarchical Clustering Cutoff Calculation"
    function_kwargs={"protocol_params":protocol_params,"condensed_distance_matrix":condensed_distance_matrix,"max_dist":max_dist}
    process_manager.add_process_internally(process_name,description,spawn_hierarchical_cutoff_calculation_function,function_kwargs,dependency = [])


def spawn_hierarchical_cutoff_calculation_function(protocol_params,condensed_distance_matrix,max_dist):
    common.print_and_flush("Spawning cutoff calculation.\n")
    hierarchicalAlgorithm = HierarchicalClusteringAlgorithm(condensed_distance_matrix)
    common.print_and_flush("Calculating cutoffs for hierarchical...")
    calculated_cutoffs_and_clusters = calculate_cutoff_numcluster_list(hierarchicalAlgorithm, None, max_dist)
    common.print_and_flush(" Done\n")
    cid = 0
    common.print_and_flush("Storing clusters...")
    for mytuple in calculated_cutoffs_and_clusters:
        clustering = mytuple[2]
        scripts_common.save_clusters_as_binary("hie_"+str(cid)+".bin",clustering)
        cid = cid + 1
    common.print_and_flush(" Done\n")

def spawn_hierarchical_with_cutoffs(protocol_params, condensed_distance_matrix,process_manager):
    hierarchicalAlgorithm = HierarchicalClusteringAlgorithm(condensed_distance_matrix)
    for c in protocol_params.hierarchical_cutoff_list:
        process_name = "Hierarchical_"+str(c)
        description = "Hierarchical clustering calculation with cutoff = "+str(c)
        function_kwargs={"algorithm":hierarchicalAlgorithm,"clustering_id":process_manager.next_process_id(),"algorithm_kwargs":{"cutoff":c}}
        process_manager.add_process_internally(process_name,description,run_algorithm,function_kwargs,dependency = [])

def spawn_gromos(protocol_params, condensed_distance_matrix,process_manager):
    ###################
    # Gromos
    ###################
    common.print_and_flush("Spawning gromos clustering.\n")
    alg = GromosAlgorithm(condensed_distance_matrix)
    for c in protocol_params.gromos_cutoff_list:
        process_name = "Gromos_"+str(c)
        description = "GROMOS Algorithm Calculation. Cutoff:"+str(c)
        function_kwargs={"algorithm":alg,"clustering_id":process_manager.next_process_id(),"algorithm_kwargs":{"cutoff":c}}
        process_manager.add_process_internally(process_name,description,run_algorithm,function_kwargs,dependency = [])

def spawn_random( protocol_params, condensed_distance_matrix,process_manager):
    ###################
    # Random
    ###################
    common.print_and_flush("Spawning random clustering.\n")
    alg = RandomClusteringAlgorithm(condensed_distance_matrix)
    for i in range(protocol_params.max_random_clusters):
        process_name = "Random_" + str(i)
        description = "Random Algorithm Calculation "+str(i)
        function_kwargs={"algorithm":alg, "clustering_id":process_manager.next_process_id(), "algorithm_kwargs":{"max_num_of_clusters":protocol_params.max_clusters}}
        process_manager.add_process_internally(process_name,description,run_algorithm,function_kwargs,dependency = [])

def spawn_kmedoids(protocol_params, condensed_distance_matrix,starting_gromos_cutoff,process_manager):
    ###################
    # KMEDOIDS
    ###################
    common.print_and_flush("Spawning k-medoids clustering.\n")
    base_cutoff = starting_gromos_cutoff
    alg = KMedoids(condensed_distance_matrix)
    for k in range(protocol_params.min_clusters,protocol_params.max_clusters+1,protocol_params.kmedoids_step):
        process_name = "KMedoids_" + str(k)
        description = "K-Medoids calculation for k = "+str( k)
        function_kwargs={"algorithm":alg, "clustering_id":process_manager.next_process_id(), "algorithm_kwargs":{"k":k,"seeding_max_cutoff":base_cutoff}}
        process_manager.add_process_internally(process_name,description,run_algorithm,function_kwargs,dependency = [])
        base_cutoff -= 0.1

def spawn_dbscan(protocol_params, condensed_distance_matrix,process_manager):
    ###################
    # DBSCAN
    ###################
    dbscan_param_pairs = []
    if len(protocol_params.dbscan_param_pairs)==0:
        common.print_and_flush( "Choosing better params for DBSCAN ... ")
        dbscan_param_pairs = dbscan_param_space_search(condensed_distance_matrix.row_length, protocol_params.max_noise, condensed_distance_matrix)
        common.print_and_flush( " Done\n")
    else:
        dbscan_param_pairs = protocol_params.dbscan_param_pairs
    
    common.print_and_flush("Spawning DBSCAN clustering.\n")
    alg = DBSCANAlgorithm(condensed_distance_matrix)
    for pair in dbscan_param_pairs:
        process_name = "DBSCAN_minpts_" + str(pair[0])+"_cutoff_"+str(pair[1])
        description = "DBSCAN calculation with minpts = " + str(pair[0])+" and  cutoff = "+str(pair[1])
        function_kwargs={"algorithm":alg, "clustering_id":process_manager.next_process_id(), "algorithm_kwargs":{"eps": pair[1], "minpts":pair[0]}}
        process_manager.add_process_internally(process_name,description,run_algorithm,function_kwargs,dependency = [])

def spawn_matrix_analysis_processes(condensed_distance_matrix,protocol_params,analysis_working_dir, process_manager):
     
    p = Process(target=distance_distribution_analysis, name="Distance Distribution Analysis", kwargs={"condensed_distance_matrix":condensed_distance_matrix,"folder_path":analysis_working_dir})
    p.description = "Distance Distribution Analysis"
    process_manager.add_process(p,p.name,[])
    
    p = Process(target=mean_distance_distribution_analysis, name="Mean Distance Distribution Analysis", kwargs={"condensed_distance_matrix":condensed_distance_matrix,"folder_path":analysis_working_dir})
    p.description = "Mean Distance Distribution Analysis"
    process_manager.add_process(p,p.name,[])
    
    for cutoff in protocol_params.mean_dist_per_element_cutoffs:
        p = Process(target=mean_distance_per_element, name="Mean Distance Per Element and Cutoff"+str(cutoff), kwargs={"condensed_distance_matrix":condensed_distance_matrix,"folder_path":analysis_working_dir,"cutoff":cutoff})
        p.description = "Mean Distance Per Element and Cutoff"+str(cutoff)
        process_manager.add_process(p,p.name,[])
    
    for cutoff in protocol_params.percent_of_elements_cutoff:
        p = Process(target=percent_of_elements_within_cutoff_analysis, name="Percent of Elements Within Cutoff"+str(cutoff), kwargs={"condensed_distance_matrix":condensed_distance_matrix,"folder_path":analysis_working_dir,"cutoff":cutoff})
        p.description = "Mean Distance Per Cutoff"+str(cutoff)
        process_manager.add_process(p,p.name,[])
    
    if len(protocol_params.k_dist_k_elem)!=0:
        p = Process(target=kdist_analysis, name="K-Dist Analysis", kwargs={"condensed_distance_matrix":condensed_distance_matrix,"folder_path":analysis_working_dir,"klist":protocol_params.k_dist_k_elem})
        p.description = "K-Dist Analysis"
        process_manager.add_process(p,p.name,[])


def run_algorithm(algorithm, algorithm_kwargs,clustering_id):
    clustering = algorithm.perform_clustering(algorithm_kwargs)
    scripts_common.save_clusters_as_binary(str(clustering_id)+".bin",clustering)
           
